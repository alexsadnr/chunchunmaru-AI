# chunchunmaru-AI

Система предсказания света на кухне умного дома, оформленная как небольшой «магический артефакт» для соревнований Kaggle.
За кулисами — честный PyTorch-пайплайн для бинарной классификации временных рядов, на сцене — дружелюбный скрипт `main.py`, делающий всё сам: от подготовки данных до сохранения `submission.csv`. 

---

## 1. Общая идея

**Цель проекта** — по данным датчиков умного дома (каждую секунду в течение нескольких дней) предсказать, включён ли свет на кухне (`Light_Kitchen`) в каждый момент времени тестового дня.

По сути:

* вход: временной ряд признаков за несколько дней (`train_data.csv`, `test_data.csv`);
* задача: бинарная классификация «0/1» по окну истории;
* модель: LSTM-классификатор над последовательностями фиксированной длины;
* выход: файл `submission.csv` в формате Kaggle-соревнования.

---

## 2. Технологический стек

Основные библиотеки:

* **Python 3.x**
* **NumPy**, **pandas** — работа с данными;
* **matplotlib**, **seaborn** — базовая визуализация кривых обучения;
* **scikit-learn** — `MinMaxScaler`, `train_test_split`, метрики;
* **PyTorch** — модель, обучение, инференс.

---

## 3. Структура проекта

Минимальный набор файлов:

* `main.py` — основной боевой скрипт:

  * загрузка данных;
  * предобработка и генерация окон;
  * определение и обучение модели;
  * инференс на тесте;
  * генерация `submission.csv`.
* `train_data.csv` — обучающая выборка с целевым признаком `Light_Kitchen`.
* `test_data.csv` — тестовая выборка без `Light_Kitchen`.
* `submission.csv` — файл с предсказаниями для отправки на Kaggle (генерируется скриптом).

---

## 4. Запуск проекта

### 4.1. Установка зависимостей

Рекомендуется использовать виртуальное окружение:

```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

При отсутствии `requirements.txt` — минимум:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn torch
```

### 4.2. Подготовка данных

Положите файлы:

* `train_data.csv`
* `test_data.csv`

в ту же директорию, где лежит `main.py`.

### 4.3. Запуск обучения и инференса

```bash
python main.py
```

Скрипт:

1. обучит модель на `train_data.csv`;
2. оценит качество на валидации;
3. сделает предсказания для `test_data.csv`;
4. сохранит итоговый файл `submission.csv` в текущей директории.

---

## 5. Формат данных

### 5.1. Обучающие данные (`train_data.csv`)

Обязательные поля:

* `timestamp` — отметка времени (строка, приводится к `datetime`);
* `Light_Kitchen` — целевой бинарный признак (0/1);
* набор числовых признаков (датчики, состояния и т.п.);
* опционально: `ID` (не обязателен в train).

Требования:

* временные ряды с шагом 1 секунда;
* все дополнительные столбцы с типом `float` / `int` считаются кандидатами в признаки.

### 5.2. Тестовые данные (`test_data.csv`)

* `timestamp` — отметка времени;
* `ID` — идентификатор строки (используется при формировании сабмишена);
* те же числовые признаки, что и в train (без `Light_Kitchen`).

---

## 6. Обработка данных

Основные шаги предобработки реализованы в `main.py`:

1. **Работа с временем**

   * `timestamp` приводится к типу `datetime`;
   * данные сортируются по времени;
   * индекс пересобирается по порядку.

2. **Определение типов признаков**

   * Выделяются **числовые** столбцы.
   * Среди них отдельно определяются **бинарные** (значения только 0/1).
   * Остальные числовые считаются **непрерывными** (continuous).

3. **Обработка аномалий и пропусков**

   * Для некоторых колонок (например, `Temperature`) значения вне разумного диапазона заменяются на `NaN`.
   * Пропуски в непрерывных признаках заполняются с помощью **экспоненциального скользящего среднего (EMA)**.
   * Затем применяется **forward fill** (протягивание вперёд) для предотвращения утечки информации из будущего.

4. **Масштабирование признаков**

   * Используется `MinMaxScaler` из `sklearn`.
   * Скейлер обучается **только на train**.
   * Затем к тому же диапазону приводятся признаки test.

Результат: два массива признаков `train_features_scaled` и `test_features_scaled`, полностью очищенных от пропусков и приведённых к диапазону [0, 1].

---

## 7. Формирование временных окон

Задача формализована как бинарная классификация по окну истории:

* длина окна `SEQ_LENGTH_BIN = 120` (120 секунд = 2 минуты);
* для каждого момента времени формируется последовательность длины 120 по всем признакам;
* целевая метка — значение `Light_Kitchen` **на следующий момент после окна**.

Процедура:

1. Из массива признаков `train_features_scaled.values` формируются последовательности `X_all` размера:

   * `[num_samples, SEQ_LENGTH_BIN, num_features]`.
2. Одновременно формируется массив меток `y_all` размера:

   * `[num_samples]`.
3. Временной ряд разбивается на:

   * обучающую выборку;
   * валидационную (например, последние 20% данных).

Для теста:

* объединяются train+test признаки во временном порядке;
* для каждой позиции тестовых данных вырезается окно длины 120 шагов назад;
* если окно «не влезает» (начало ряда) — используется нулевой паддинг, далее маскирование и до-заполнение средним.

---

## 8. Архитектура модели

Используется простая рекуррентная архитектура на базе **LSTM**:

```text
Input (batch_size, seq_len=120, input_size=N_features)
  → LSTM(num_layers=2, hidden_size=32)
  → Linear(hidden_size → 1 логит)
```

Особенности реализации:

* `batch_first=True` в LSTM (данные формата `[batch, seq, features]`);
* для каждого батча инициализируются `h0`, `c0` нулями;
* на выход берётся только последний временной шаг (`out[:, -1, :]`);
* выходной слой `Linear` даёт **логит**, далее используется `BCEWithLogitsLoss`.

Гиперпараметры (по умолчанию):

* `HIDDEN_SIZE = 32`
* `NUM_LAYERS = 2`
* `BATCH_SIZE = 128`
* `SEQ_LENGTH_BIN = 120`
* `NUM_EPOCHS` — задаётся в коде (например, 3 для быстрого прогона);
* оптимизатор: `Adam`, `lr = 1e-3`.

---

## 9. Обучение и валидация

### 9.1. Цикл обучения

Функция `training_loop` выполняет:

1. проход по обучающим батчам:

   * прямой проход;
   * вычисление функции потерь (`BCEWithLogitsLoss`);
   * обратное распространение;
   * обрезка градиентов (`clip_grad_norm_`) для устойчивости;
   * шаг оптимизатора;
2. оценку на валидационной выборке;
3. логирование значений `Train Loss` и `Val Loss` по эпохам;
4. сохранение списков `train_losses`, `val_losses`.

### 9.2. Оценка на валидации

Функция `evaluate_model`:

* прогоняет модель на валидационном даталоудере;
* применяет `sigmoid` к логитам, получая вероятности;
* считает бинарные предсказания (порог 0.5);
* оценивает качество через `classification_report` (`precision/recall/f1`).

---

## 10. Инференс и формирование сабмишена

Для тестовых данных:

1. Строится набор окон `X_test_sequences` для всех строк `test_data`.
2. Для каждого окна:

   * модель выдаёт логит;
   * применяется `sigmoid` → вероятность включения света.
3. Окна, где не хватило истории (первые минуты ряда), заполняются:

   * вероятностью среднего по валидным позициям;
   * это уменьшает артефакты на границе.

Финальный шаг:

* вероятности превращаются в метки 0/1 (порог 0.5);

* собирается DataFrame:

  ```text
  ID,Light_Kitchen
  0,0
  1,1
  ...
  ```

* сохраняется в `submission.csv` (без индекса).

---

## 11. Расширение и доработка

Возможные направления улучшения:

1. **Новые архитектуры**

   * SimpleRNN / GRU / комбинированные модели;
   * сверточные модели для временных рядов;
   * трансформеры для длинных историй.

2. **Более сложная предобработка**

   * генерация новых признаков (агрегаты по окну, лаги, скользящие статистики);
   * учёт цикличности времени (часы, дни недели).

3. **Тюнинг гиперпараметров**

   * длина окна `SEQ_LENGTH_BIN`;
   * размер скрытого состояния LSTM;
   * число слоёв, dropout;
   * подбор порога классификации.

4. **Сохранение и загрузка чекпоинтов**

   * `torch.save()` / `torch.load()` для воспроизводимости.

---

## 12. Краткое резюме

`chunchunmaru-AI` — аккуратный пайплайн для решения задачи бинарной классификации по временным рядам умного дома:

* минимальные внешние зависимости;
* последовательная предобработка;
* LSTM-модель для учёта истории;
* автоматическая генерация сабмишена для Kaggle.

Всё, что требуется «герою», — подготовить данные, запустить `main.py` и забрать свой `submission.csv`.
